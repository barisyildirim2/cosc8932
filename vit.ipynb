{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79620c5d-9742-44c9-b2e4-e28c1d3a2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"./data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/tutorial15\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38021438-e83c-43fd-9392-b6e9677847e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"tutorial15/ViT.ckpt\", \"tutorial15/tensorboards/ViT/events.out.tfevents.ViT\",\n",
    "                    \"tutorial5/tensorboards/ResNet/events.out.tfevents.resnet\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name.split(\"/\",1)[1])\n",
    "    if \"/\" in file_name.split(\"/\",1)[1]:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74a692-bad2-49d6-a235-819bf020d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "pl.seed_everything(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "# Visualize some examples\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6caeb3c2-fd33-4b6a-8ed3-726148cc301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4424ba-82d4-4758-af56-415d7017b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff4dcf5-0818-44c0-8461-cb4362f9753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network \n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, \n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff1f368-1fe3-4407-8974-57cdf72a00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and \n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "        \n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9234d0c6-24e2-48c8-befb-3d5db3401fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]   \n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log(f'{mode}_loss', loss)\n",
    "        self.log(f'{mode}_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa51232-e383-450b-afb4-1a4aa36ae60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"), \n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=180,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45712f49-9c36-4af0-a02a-d30ad321ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = train_model(model_kwargs={\n",
    "                                'embed_dim': 256,\n",
    "                                'hidden_dim': 512,\n",
    "                                'num_heads': 8,\n",
    "                                'num_layers': 6,\n",
    "                                'patch_size': 4,\n",
    "                                'num_channels': 3,\n",
    "                                'num_patches': 64,\n",
    "                                'num_classes': 10,\n",
    "                                'dropout': 0.2\n",
    "                            },\n",
    "                            lr=3e-4)\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b99fa04e-28f3-43fc-8a6f-df0c65bd5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tensorboard_path = '/opt/anaconda3/envs/vit/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] = tensorboard_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f8664-9ab6-4f12-9ae9-c110f84ec3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../saved_models/tutorial15/tensorboards/ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302f958-80b1-427d-bcf3-a974f1bbca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../saved_models/tutorial15/tensorboards/ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a9cf4-922c-4d02-9910-df7093e159e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the model with 2 new images. \n",
    "\n",
    "def prepare_image(image_path):\n",
    "    # Define the transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # Resize to 32x32 pixels (CIFAR-10 size)\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(\n",
    "            mean=[0.49139968, 0.48215841, 0.44653091],  # CIFAR-10 mean\n",
    "            std=[0.24703223, 0.24348513, 0.26158784]  # CIFAR-10 std\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "# My images\n",
    "image_path1 = 'car1.jpg'\n",
    "image_path2 = 'ship1.jpg'\n",
    "\n",
    "# Prepare the images\n",
    "img1_tensor = prepare_image(image_path1)\n",
    "img2_tensor = prepare_image(image_path2)\n",
    "\n",
    "# Print the shapes - commented since not presentable\n",
    "# print(f\"Shape of image 1: {img1_tensor.shape}\")\n",
    "# print(f\"Shape of image 2: {img2_tensor.shape}\")\n",
    "# print(\"----------------------\")\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Function to process and print predictions for a single image\n",
    "def process_image(img_tensor, image_number):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    \n",
    "    # Get the predicted class index\n",
    "    _, predicted = output.max(1)\n",
    "    \n",
    "    # Get the class name and confidence\n",
    "    class_name = classes[predicted.item()]\n",
    "    confidence = torch.softmax(output, dim=1)[0][predicted.item()].item()\n",
    "    \n",
    "    print(f\"Image {image_number} is predicted as: {class_name} with confidence {confidence:.2%}\")\n",
    "    \n",
    "    # Print top 3 predictions\n",
    "    top3 = torch.topk(torch.softmax(output, dim=1)[0], 3)\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for j in range(3):\n",
    "        print(f\"  {classes[top3.indices[j]]}: {top3.values[j]:.2%}\")\n",
    "    print(\"----------------------\")\n",
    "    print()\n",
    "\n",
    "# Process each image individually\n",
    "process_image(img1_tensor, 1)\n",
    "process_image(img2_tensor, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
